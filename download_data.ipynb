{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipavlopoulos/ppast-gr/blob/main/download_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF SCRAPER"
      ],
      "metadata": {
        "id": "sjBL712Mwrv9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i5RB09Dvzcg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from selenium import webdriver\n",
        "from time import sleep\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Change parameters:\n",
        "var = set to the number of files existing\n",
        "endings[_:_] = from number of files existing + 3\n",
        "if int(num[0]) > __ = greater than number of files existing\n",
        "\n",
        "\"\"\"\n",
        "def pdf_scraper(index, download_folder_path):\n",
        "  failed_access = []\n",
        "  crd_list = []\n",
        "\n",
        "  first = \"https://digitallib.parliament.gr/main.asp?current=\"\n",
        "  endings = [i for i in range(12256712, 12284152)] #the interval of interest\n",
        "  var = index   #set var equal to the number of files already existing\n",
        "  dir_check = 0\n",
        "  crd = 0\n",
        "\n",
        "  for i in tqdm(endings[var+3:var+100]):\n",
        "\n",
        "    url = first + str(i)\n",
        "    print(url)\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    download_folder = download_folder_path#\"/content/drive/MyDrive/Downloads_Colab\"\n",
        "    profile = {\"plugins.plugins_list\": [{\"enabled\": False,\n",
        "                                        \"name\": \"Chrome PDF Viewer\"}],\n",
        "            \"download.default_directory\": download_folder,\n",
        "            \"download.extensions_to_open\": \"\",\n",
        "            \"plugins.always_open_pdf_externally\": True}\n",
        "    options.add_experimental_option(\"prefs\", profile)\n",
        "\n",
        "    driver = webdriver.Chrome('chromedriver',chrome_options = options)\n",
        "    #IMPORTANT\n",
        "    driver.get(url)\n",
        "    dir_check+=1\n",
        "    time.sleep(4)\n",
        "    driver.close()\n",
        "    #IF FAILED ACCESS\n",
        "    if len(os.listdir(download_folder)) <dir_check:\n",
        "      failed_access.append(url)\n",
        "      print(f'Number of files failed: {len(failed_access)}\\nItems in dir: {len(os.listdir(download_folder))}\\n.')\n",
        "      var+=1\n",
        "      dir_check-=1\n",
        "\n",
        "    for file in os.listdir(download_folder):\n",
        "\n",
        "      if file == \"main.pdf\":\n",
        "        file = download_folder + \"/\" + file\n",
        "        var += 1\n",
        "        name = download_folder + \"/\" + \"main\" + str(var) + \".pdf\"\n",
        "        os.rename(file, name)\n",
        "        print(f'Folder Done\\n\\nN. {i}\\n\\nFile : {var}\\nFiles in dir: {len(os.listdir(download_folder))}')\n",
        "    #IF CORRUPTED FILE\n",
        "    if 'main.pdf.crdownload' in os.listdir(download_folder):\n",
        "        var+=1\n",
        "        print(f'.\\nWARNING!!! crdownload for file:\\nFile: {url}\\n')\n",
        "        crd_list.append(url)\n",
        "        os.remove('/content/drive/MyDrive/Downloads_Colab/main.pdf.crdownload')"
      ],
      "metadata": {
        "id": "DuOsqS7iwqT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cropping"
      ],
      "metadata": {
        "id": "dsmO31Fbw2Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pdf_horizontal_crop(download_folder):\n",
        "  for pdf in os.listdir(download_folder):# folder with PDF files\n",
        "    num = re.findall(r'\\d+', pdf )\n",
        "    if int(num[0]) > 0: #set int(num[0]) greater than the existing files.\n",
        "\n",
        "      cur_file = download_folder + \"\\\\\" + pdf\n",
        "      reader = PdfFileReader(cur_file)\n",
        "      writer = PdfFileWriter()\n",
        "\n",
        "      for page in reader.pages:\n",
        "        page.cropbox.lower_left = ((10,362))\n",
        "        page.cropbox.upper_right = ((602,782))\n",
        "        writer.add_page(page)\n",
        "\n",
        "      with open(cur_file,'wb') as fp:\n",
        "        writer.write(fp)\n",
        "\n",
        "def pdf_vertical_crop(download_folder, output_folder):\n",
        "  for pdf in tqdm(os.listdir(download_folder)):# folder with PDF files\n",
        "    time.sleep(0.2)\n",
        "    if 'main' in pdf:\n",
        "\n",
        "      num = re.findall(r'\\d+', pdf )\n",
        "      cur_file = download_folder + '\\\\' + pdf\n",
        "      reader = PdfFileReader(cur_file, 'r')\n",
        "      writer = PdfFileWriter()\n",
        "      for page in reader.pages:\n",
        "\n",
        "        page.cropbox.lower_left = ((10,362))\n",
        "        page.cropbox.upper_right = ((301,782))\n",
        "        half_file = output_folder + '\\\\' + 'main' + str(num[0]) + '_1' + '.pdf'\n",
        "        writer.add_page(page)\n",
        "        with open(half_file, 'wb+') as half_pdf:\n",
        "          writer.write(half_pdf)\n",
        "\n",
        "        page.cropbox.lower_left = ((301,362))\n",
        "        page.cropbox.upper_right = ((610,782))\n",
        "        half_file = output_folder + '\\\\' + 'main' + str(num[0]) + '_2' + '.pdf'\n",
        "        writer.add_page(page)\n",
        "        with open(half_file, 'wb+') as half_pdf:\n",
        "          writer.write(half_pdf)"
      ],
      "metadata": {
        "id": "pmtZqD0NzkKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JPG Extraction"
      ],
      "metadata": {
        "id": "KsAKkI4U2lcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import io\n",
        "from PIL import Image\n",
        "import os\n",
        "from time import sleep\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Pk45Dmf42rOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jpg_extraction(input_folder, output_folder):\n",
        "\n",
        "  list_dir = os.listdir(input_folder)\n",
        "  for i in tqdm(list_dir):\n",
        "    name = i.replace('main','').replace('.pdf','')\n",
        "    file = input_folder + '/' + i\n",
        "    doc = fitz.open(file)\n",
        "\n",
        "    for page in doc:\n",
        "      pix = page.get_pixmap(matrix=fitz.Identity, dpi=300,colorspace=fitz.csRGB, clip=None, alpha=True, annots=True)\n",
        "      pix.save(f\"{output_folder}/main{name}.jpg\")  # save file\n",
        "    sleep(0.02)"
      ],
      "metadata": {
        "id": "RN6yA3q_2w6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XML Parser"
      ],
      "metadata": {
        "id": "Yk-5ENOw3wNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom.minidom import parseString\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from natsort import natsorted"
      ],
      "metadata": {
        "id": "Dm-0hNy73zRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xml_parser(xml_path, txt_path):\n",
        "\n",
        "  dir = os.listdir(xml_path)\n",
        "  for file in tqdm(dir): #accessing xml file\n",
        "    xml_file = xml_path + '/' + file\n",
        "    mytree = ET.parse(xml_path + '/' + file)\n",
        "    myroot = mytree.getroot()\n",
        "    with open(xml_file,'r') as xml:\n",
        "      data = xml.read()\n",
        "      dom = parseString(data)\n",
        "      textRegion = len(dom.getElementsByTagName('TextRegion')) #counting the text regions\n",
        "\n",
        "    txt_file = txt_path + '/' + file.replace('xml', 'txt')  #creating the txt file of the page\n",
        "    f = open(txt_file,'w')\n",
        "    count_Region = 0\n",
        "    count_Line = 1\n",
        "\n",
        "\n",
        "    while count_Region < textRegion:\n",
        "      count_Region += 1\n",
        "      textLine_list = [i for i in myroot[1][count_Region] if i.tag[-8:] == 'TextLine'] #counting lines per region\n",
        "      while count_Line < len(textLine_list) + 1:\n",
        "        line = str(myroot[1][count_Region][count_Line][-1][0].text)\n",
        "        f.write(f'{line} ')\n",
        "\n",
        "        count_Line+=1\n",
        "      count_Line = 1\n",
        "    f.close"
      ],
      "metadata": {
        "id": "IaCAZ3iD4K2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset"
      ],
      "metadata": {
        "id": "y2oyBOAnAd1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from natsort import natsorted"
      ],
      "metadata": {
        "id": "TolAd3iGAh5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create three lists\n",
        "Text_list = list with text from each page\n",
        "year_list = list of the period (found in the website of the Greek Parliament)\n",
        "committee_list = list of the committee (found in the website of the Greek Parliament)\n",
        "Combine the lists to one dataframe\n",
        "\"\"\"\n",
        "def csv_creator(txt_path):\n",
        "  text_list = [] #list with text files\n",
        "  year_list = [] # list with period\n",
        "  committee_list = [] # list of committee\n",
        "  for i in natsorted(os.listdir(txt_path)):\n",
        "    num = re.findall(r'\\d+', i)\n",
        "    txt = txt_path + '/' + i\n",
        "    # add text to list\n",
        "    with open(txt, 'r') as f:\n",
        "      text = f.read()\n",
        "      f.close()\n",
        "    text_list.append(text)\n",
        "\n",
        "    # add period and committee to list based on id\n",
        "    if int(num[0]) <= 141:\n",
        "      year_list.append('13/05/1946 - 20/06/1946')\n",
        "      committee_list.append(\"ΑΝΑΘΕΩΡΗΤΙΚΗ - ΣΥΝΟΔΟΣ Α' (T.1)\")\n",
        "    elif 141 < int(num[0]) <= 306:\n",
        "      year_list.append('21/06/1946 - 31/07/1946')\n",
        "      committee_list.append(\"ΑΝΑΘΕΩΡΗΤΙΚΗ - ΣΥΝΟΔΟΣ Α' (T.2)\")\n",
        "    elif 306 < int(num[0]) <= 545:\n",
        "      year_list.append('01/10/1946 - 27/02/1947')\n",
        "      committee_list.append(\"ΑΝΑΘΕΩΡΗΤΙΚΗ - ΣΥΝΟΔΟΣ Α' (T.3)\")\n",
        "    elif 545 < int(num[0]) <= 802:\n",
        "      year_list.append('21/04/1947 - 13/09/1947')\n",
        "      committee_list.append(\"ΑΝΑΘΕΩΡΗΤΙΚΗ - ΣΥΝΟΔΟΣ Α' (T.4)\")\n",
        "    elif 802 < int(num[0]) <= 1093:\n",
        "      year_list.append('09/08/1946 - 18/04/1947')\n",
        "      committee_list.append(\"ΑΝΑΘΕΩΡΗΤΙΚΗ - ΣΥΝΟΔΟΣ Α' - ΕΠΙΤΡΟΠΗ (T.1)\")\n",
        "    elif 1093 < int(num[0]) <= 1319:\n",
        "      year_list.append('08/07/1947 - 31/10/1947')\n",
        "      committee_list.append(\"ΑΝΑΘΕΩΡΗΤΙΚΗ - ΣΥΝΟΔΟΣ Α' - ΕΠΙΤΡΟΠΗ (T.2)\")\n",
        "    elif 1319 < int(num[0]) <= 1589:\n",
        "      year_list.append('01/11/1947 - 23/01/1948')\n",
        "      committee_list.append(\"ΑΝΑΘΕΩΡΗΤΙΚΗ - ΣΥΝΟΔΟΣ Β' (T.1)\")\n",
        "\n",
        "  columns = ['id', \"period\", 'comitee', 'text']\n",
        "  minutes_df = pd.DataFrame({'id': id_list,\n",
        "                              'period' : year_list,\n",
        "                              'commitee' : committee_list,\n",
        "                              'text': text_list},\n",
        "                              columns = columns)\n",
        "\n",
        "  return minutes_df"
      ],
      "metadata": {
        "id": "oKKcSW6HAj6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegEx"
      ],
      "metadata": {
        "id": "d3SIgTh8L5Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from natsort import natsorted\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "CBqNZ6R3L8Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regex_check(csv_file_path):\n",
        "  minutes_df = pd.read_csv(csv_file_path) #path of csv with image id, period, committee and text\n",
        "\n",
        "  #add columns for the data extracted by RegEx\n",
        "  minutes_df['Date'] = ''\n",
        "  minutes_df['Orator'] = ''\n",
        "  minutes_df['Applause'] = ''\n",
        "  i = 0\n",
        "  for row in minutes_df.text:\n",
        "    if isinstance(row, float) == False:\n",
        "      date = re.findall(r'([Α-ΩΆΈΎΊΌΏἈἘἸὈὨᾺῈῪῚῸῺἊἚἺὊὪα-ωάέύίήόὶὺὰὲὸὴῖῦᾶῆ]+\\s+[0-9]+\\s+[Α-ΩΆΈΎΊΌΏἈἘἸὈὨᾺῈῪῚῸῺἊἚἺὊὪα-ωάέύίήώόὶὺὰὲὼὸῖῦᾶῆϊϋΐΰ]+\\s+[0-9]+\\.*)', row) #RegEx for date\n",
        "      if date:\n",
        "          if date[0] in row:\n",
        "            minutes_df.loc[i, 'Date'] = date[0]\n",
        "    i += 1\n",
        "\n",
        "  rows = 0\n",
        "  for text in tqdm(minutes_df.text.tolist()):\n",
        "    orators = []\n",
        "    if isinstance(text, float) == False:\n",
        "      orator = re.findall(r\"[\\s\\.]*([Α-ΩἈἉἊἘἙἚἸἹἺὈὉὊ'ΥὙὛὨὩὫ]\\s([α-ωάᾶὰἆέὲύϋῦὺὖὒίϊῖὶἶόὸώῶὼὦὢ]{1,3}\\s){1,8})[\\s\\.]*\", text) #RegEx for Orator\n",
        "      if len(orator) >= 1:\n",
        "        orators.append(orator[0])\n",
        "\n",
        "    minutes_df.loc[rows, 'Orator'] = ','.join(','.join(tup) for tup in orators)\n",
        "    rows += 1\n",
        "\n",
        "  rows = 0\n",
        "  for text in tqdm(minutes_df.text.tolist()):\n",
        "    claps = []\n",
        "    if isinstance(text, float) == False:\n",
        "      clap = re.findall(r\"(\\(.*(κρο).*\\))\", text) #RegEx for Applause\n",
        "      if 15 > len(clap) >= 1:\n",
        "        claps.append(clap[0])\n",
        "\n",
        "    minutes_df.loc[rows, 'Applause'] = ','.join(','.join(tup) for tup in claps)\n",
        "    rows += 1"
      ],
      "metadata": {
        "id": "Ic5ND9IVMA1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}